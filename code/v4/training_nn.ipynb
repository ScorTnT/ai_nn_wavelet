{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f941dd69",
   "metadata": {},
   "source": [
    "# V4-NN: Heart Sound Classification using a Neural Network\n",
    "\n",
    "이 노트북은 **전처리된 웨이브릿 특성**을 사용하여 심장 소리를 분류하는 **신경망(Multi-Layer Perceptron)**을 훈련합니다.\n",
    "\n",
    "**프로젝트 목표:**\n",
    "1. 1차원 심장소리 데이터에서 여러 특징 추출 방식의 정확도 비교\n",
    "2. 원본 .wav 데이터 vs 웨이브릿 변환 데이터의 성능 비교\n",
    "3. 웨이브릿 변환을 통한 정확도 향상 효과 검증\n",
    "\n",
    "**작업 흐름:**\n",
    "1. **설정**: 전처리된 데이터 경로 설정\n",
    "2. **데이터 로딩**: `wavelet_v4/` 디렉토리에서 웨이브릿 계수와 라벨 로드\n",
    "3. **데이터 준비**: 훈련/테스트 세트 분할 및 특성 스케일링\n",
    "4. **모델 정의**: TensorFlow/Keras를 사용한 다층 신경망 구축\n",
    "5. **모델 훈련 및 평가**: 컴파일, 훈련, 성능 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada02776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6919fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration ---\n",
    "# V4 웨이브릿 데이터 경로\n",
    "WAVELET_DATA_DIR = '/workspace/wavelet_v4/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bf3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Data Loading ---\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "print(f\"Loading V4 wavelet data from {WAVELET_DATA_DIR}...\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(WAVELET_DATA_DIR):\n",
    "    print(f\"Error: Directory not found at {os.path.abspath(WAVELET_DATA_DIR)}\")\n",
    "    print(\"Please run the preprocessing script first: python preprocess_wavelet.py\")\n",
    "else:\n",
    "    files = os.listdir(WAVELET_DATA_DIR)\n",
    "    npz_files = [f for f in files if f.endswith('.npz')]\n",
    "    \n",
    "    if not npz_files:\n",
    "        print(f\"No .npz files found in {WAVELET_DATA_DIR}. Please run preprocessing first.\")\n",
    "    else:\n",
    "        print(f\"Found {len(npz_files)} .npz files\")\n",
    "        \n",
    "        # Load all wavelet feature files\n",
    "        for file_name in npz_files:\n",
    "            path = os.path.join(WAVELET_DATA_DIR, file_name)\n",
    "            try:\n",
    "                data = np.load(path)\n",
    "                \n",
    "                # Load features and labels using the new format\n",
    "                if 'features' in data and 'label' in data:\n",
    "                    X.append(data['features'])\n",
    "                    y.append(data['label'])\n",
    "                else:\n",
    "                    print(f\"Warning: Expected keys 'features' and 'label' not found in {file_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_name}: {e}\")\n",
    "        \n",
    "        if X:\n",
    "            # Convert to numpy arrays\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "            \n",
    "            print(f\"\\n=== V4 Data Loading Complete ===\")\n",
    "            print(f\"Successfully loaded {len(X)} samples.\")\n",
    "            print(f\"Feature matrix shape: {X.shape}\")\n",
    "            print(f\"Label distribution: Normal (0): {np.sum(y == 0)}, Abnormal (1): {np.sum(y == 1)}\")\n",
    "            print(f\"Class balance: {np.sum(y == 0)/(len(y))*100:.1f}% normal, {np.sum(y == 1)/(len(y))*100:.1f}% abnormal\")\n",
    "        else:\n",
    "            print(\"No valid data could be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e97ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Data Preparation ---\n",
    "if len(X) > 0:\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(f'=== V4 Data Preparation Complete ===')\n",
    "    print(f'Training set shape: {X_train_scaled.shape}')\n",
    "    print(f'Test set shape: {X_test_scaled.shape}')\n",
    "    print(f'Training label distribution: Normal: {np.sum(y_train == 0)}, Abnormal: {np.sum(y_train == 1)}')\n",
    "    print(f'Test label distribution: Normal: {np.sum(y_test == 0)}, Abnormal: {np.sum(y_test == 1)}')\n",
    "else:\n",
    "    print(\"Skipping model training as no data was loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964913bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Model Definition ---\n",
    "# We define the model inside a condition to avoid errors if data loading failed\n",
    "if len(X) > 0:\n",
    "    model = Sequential([\n",
    "        # Input layer: Dense with ReLU activation. The input_shape must match the number of features.\n",
    "        Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        # Dropout layer to prevent overfitting\n",
    "        Dropout(0.5), \n",
    "        # Hidden layer\n",
    "        Dense(64, activation='relu'),\n",
    "        # Another dropout layer\n",
    "        Dropout(0.3),\n",
    "        # Another hidden layer\n",
    "        Dense(32, activation='relu'),\n",
    "        # Final dropout\n",
    "        Dropout(0.2),\n",
    "        # Output layer: Dense with a single neuron and sigmoid activation for binary classification\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    print(\"=== V4 Model Architecture ===\")\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0513c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Model Training & Evaluation ---\n",
    "if len(X) > 0:\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Define early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "\n",
    "    print(\"=== Starting V4 Model Training ===\")\n",
    "    history = model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        epochs=150, # Increased epochs for better training\n",
    "        batch_size=32, \n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== V4 Final Model Evaluation ===\")\n",
    "    loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Test Loss: {loss:.4f}\")\n",
    "\n",
    "    # Make predictions for detailed analysis\n",
    "    y_pred_proba = model.predict(X_test_scaled)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\n=== Detailed Classification Report ===\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Normal', 'Abnormal']))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Normal', 'Abnormal'], \n",
    "                yticklabels=['Normal', 'Abnormal'])\n",
    "    plt.title('V4 Wavelet Features - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "    # --- Plotting Training History ---\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('V4 Model Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Loss plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('V4 Model Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n=== V4 Training Summary ===\")\n",
    "    print(f\"Training stopped at epoch: {len(history.history['loss'])}\")\n",
    "    print(f\"Best validation accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"Final test accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
