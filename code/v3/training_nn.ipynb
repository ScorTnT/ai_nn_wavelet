{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V3-NN: Heart Sound Classification using a Neural Network\n",
    "\n",
    "This notebook trains a **Neural Network (Multi-Layer Perceptron)** to classify heart sounds using **pre-processed wavelet features** stored in the `wavelet/` directory.\n",
    "\n",
    "The workflow is as follows:\n",
    "\n",
    "1.  **Configuration**: Set up the path to the pre-processed data.\n",
    "2.  **Data Loading**: Load the `.npz` files containing wavelet coefficients and corresponding labels.\n",
    "3.  **Data Preparation**: Split the data into training and testing sets and apply feature scaling.\n",
    "4.  **Model Definition**: Define and build a sequential neural network with multiple layers using TensorFlow/Keras.\n",
    "5.  **Model Training & Evaluation**: Compile, train, and evaluate the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-29 10:21:58.584994: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-29 10:21:58.595708: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-29 10:21:58.660724: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-07-29 10:21:58.707908: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753784518.759107    7294 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753784518.771807    7294 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753784518.882327    7294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753784518.882360    7294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753784518.882362    7294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753784518.882363    7294 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-29 10:21:58.894840: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration ---\n",
    "# Adjust the path based on the Docker container's file structure if needed\n",
    "WAVELET_DATA_DIR = '/workspace/wavelet_v2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from /workspace/wavelet/...\n",
      "Examining file: a0001.npz\n",
      "Available keys: ['cA', 'cD']\n",
      "Key 'cA': shape = (35669,), dtype = float32\n",
      "Key 'cD': shape = (35669,), dtype = float32\n",
      "\n",
      "Attempting to load data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3240,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 67\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWarning: Expected keys \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcA\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcD\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# Convert lists to numpy arrays\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(X)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3240,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# --- 2. Data Loading ---\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "print(f\"Loading data from {WAVELET_DATA_DIR}...\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.isdir(WAVELET_DATA_DIR):\n",
    "    print(f\"Error: Directory not found at {os.path.abspath(WAVELET_DATA_DIR)}\")\n",
    "    print(\"Please run the preprocessing script first: python preprocess_wavelet.py\")\n",
    "else:\n",
    "    files = os.listdir(WAVELET_DATA_DIR)\n",
    "    npz_files = [f for f in files if f.endswith('.npz')]\n",
    "    \n",
    "    if not npz_files:\n",
    "        print(f\"No .npz files found in {WAVELET_DATA_DIR}. Please run preprocessing first.\")\n",
    "    else:\n",
    "        print(f\"Found {len(npz_files)} .npz files\")\n",
    "        \n",
    "        # Load all wavelet feature files\n",
    "        for file_name in npz_files:\n",
    "            path = os.path.join(WAVELET_DATA_DIR, file_name)\n",
    "            try:\n",
    "                data = np.load(path)\n",
    "                \n",
    "                # Load features and labels using the new format\n",
    "                if 'features' in data and 'label' in data:\n",
    "                    X.append(data['features'])\n",
    "                    y.append(data['label'])\n",
    "                else:\n",
    "                    print(f\"Warning: Expected keys 'features' and 'label' not found in {file_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_name}: {e}\")\n",
    "        \n",
    "        if X:\n",
    "            # Convert to numpy arrays\n",
    "            X = np.array(X)\n",
    "            y = np.array(y)\n",
    "            \n",
    "            print(f\"Successfully loaded {len(X)} samples.\")\n",
    "            print(f\"Feature matrix shape: {X.shape}\")\n",
    "            print(f\"Label distribution: Normal (0): {np.sum(y == 0)}, Abnormal (1): {np.sum(y == 1)}\")\n",
    "            print(f\"Class balance: {np.sum(y == 0)/(len(y))*100:.1f}% normal, {np.sum(y == 1)/(len(y))*100:.1f}% abnormal\")\n",
    "        else:\n",
    "            print(\"No valid data could be loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Data Preparation ---\n",
    "if len(X) > 0:\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    print(f'Training set shape: {X_train_scaled.shape}')\n",
    "    print(f'Test set shape: {X_test_scaled.shape}')\n",
    "else:\n",
    "    print(\"Skipping model training as no data was loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Model Definition ---\n",
    "# We define the model inside a condition to avoid errors if data loading failed\n",
    "if len(X) > 0:\n",
    "    model = Sequential([\n",
    "        # Input layer: Dense with ReLU activation. The input_shape must match the number of features.\n",
    "        Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "        # Dropout layer to prevent overfitting\n",
    "        Dropout(0.5), \n",
    "        # Hidden layer\n",
    "        Dense(32, activation='relu'),\n",
    "        # Another dropout layer\n",
    "        Dropout(0.5),\n",
    "        # Output layer: Dense with a single neuron and sigmoid activation for binary classification\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Model Training & Evaluation ---\n",
    "if len(X) > 0:\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Define early stopping to prevent overfitting\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    print(\"Starting model training...\")\n",
    "    history = model.fit(\n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        epochs=100, # Increased epochs, with early stopping\n",
    "        batch_size=32, \n",
    "        validation_data=(X_test_scaled, y_test),\n",
    "        callbacks=[early_stopping],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"--- Final Model Evaluation ---\")\n",
    "    loss, accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "    # --- Plotting Training History ---\n",
    "    pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1) # Set the y-axis range to [0, 1]\n",
    "    plt.title('Model Training History')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Value')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
